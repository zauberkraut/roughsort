\documentclass[letterpaper, 12pt]{article}
\usepackage[letterpaper]{geometry}
\usepackage{amsmath, amssymb, url, rotating, sectsty, indentfirst}
\usepackage[doublespacing]{setspace}
\usepackage[strings]{underscore}

\sectionfont{\fontsize{12}{15}\selectfont}

\let\supercite\cite
\renewcommand{\cite}[1]{\textnormal{~\supercite{#1}}}

\title{Parallel Sorting of Roughly-Sorted Sequences\\CSCI 5172 Fall '16 Project}
\author{Anthony Pfaff, Jason Treadwell}

\begin{document}
\maketitle

Roughsort is a sorting algorithm that exploits the \textit{radius} of a sequence to beat the linearithmic
  lower runtime bound of the comparison sort family of algorithms.
Its structure invites implementation using parallel execution.
Here we present a parallel Roughsort implementation using Nvidia's CUDA platform for heterogeneous GPGPU programming and
  interpret our results.

\section{The Array Sorting Problem}
Sorting the elements of an array is among the most classic problems of computer science, often
  considered to be the most fundamental algorithmic problem\cite{clrs}.
Not merely just for arranging data to make it easier to read, sorting algorithms are frequently employed to render graphics
  and to improve the performance of other algorithms (e.g., set intersection or finding the unique elements of a list).

The \textit{comparison} sorts are an indispensable family of sorting algorithms containing such classics as Heapsort,
  Mergesort and Quicksort.
True to its name, algorithms in this family determine how to reorder a sequence by comparing its elements against each other
  according to some natural or explicitly-provided ordering;
  for the sake of simplicity, we shall only consider the sorting of integral elements in nondecreasing order, backed by
  random-access array storage\footnote{Sequential-access storage demands distinct and divergent design considerations.}.
A well-established result is the ``linearithmic'' $\Omega(n \lg n)$ runtime lower bound for the general problem of sorting
  an array of $n$ elements by comparison; algorithms such as Mergesort achieve $\Theta(n \lg n)$ runtime (and are thus
  asymptotically optimal) whereas Quicksort, while usually outperforming Mergesort over arrays that fit in memory,
  has quadratic-runtime pathological cases.

Comparison-based sorting algorithms, such as Mergesort, often have divide-and-conquer structures that can easily and
  substantially be sped up by using \textit{nested parallelism} and other means of parallel execution\cite{clrs}.

\section{Sorting Roughly-Sorted Sequences}
The optimal $\Omega(n \lg n)$ runtime bound of comparison sorting can be beaten when additional analysis is performed on the
  input sequence\cite{clrs}.
An algorithm by Altman and Igarashi, which we call \textit{Roughsort}, improves upon this lower bound by exploiting the degree
  to which the unprocessed input array is partially sorted\cite{altman89}.
Specifically, Roughsort sorts an array in $\Theta(n \lg k)$ time, where $k$ is the \textit{radius} of the input sequence.

A sequence $A = \{a_0, a_1, \cdots, a_{n-1}\}$ is $k$-sorted if it satisfies
$$a_i \leq a_j \,,\quad i < j - k \quad\forall\; 0 \leq i \leq j \leq n-1\,,$$
where a 0-sorted sequence is fully-sorted.
The radius of $A$ is the smallest such $k$ for which $A$ is $k$-sorted.
Alternatively, and perhaps more usefully, the radius of $A$ measures how far away its most out-of-order element is from its
  position in the fully-sorted array:
$$\text{radius}(A) = \max\{j - i \mid j > i,\, a_i > a_j \} \,.$$

Roughsort exploits the radius of $A$ by using a \textit{halving} algorithm that takes a $2l$-sorted sequence and partially
  sorts it into an $\lfloor l - 1 \rfloor$-sorted sequence.
By repeatedly halving the sortedness of $A$, we fully sort it in $\lg k$ runs, whence the runtime.
The radius of $A$ must therefore be given \textit{a priori} or determined from the input
  in order to effectively perform the algorithm.

Some applications involve sorted arrays that are updated or extended in such a way as to only slightly perturb the ordering
  (i.e., the radius $k$ of an out-of-order sequence is small)\cite{altman89}.
In such cases, Roughsort could be employed to quickly resort the sequences faster than by using a full $\Theta(n \lg n)$
  comparison sort.
Since an unsorted sequence of length $n$ can be at most $(n - 1)$-sorted, the runtime of Roughsort is
  $\Theta(n \lg k) = O(n \lg n)$ and is thus asymptotically-optimal among comparison-based sorting algorithms.

Similar to other comparison sorts, Roughsort has divide-and-conquer characteristics that support parallel execution on various
  multiprocessor models\cite{altman89, altman90}.

\section{Sequential Implementation}
As noted, Roughsort must know the radius $k$ of the input array $A = \{a_0, a_1, \cdots, a_{n-1}\}$
  in order to completely sort it.
If not known, the radius can be determined in linear time\cite{altman89}, preserving the $\Theta(n \lg k)$ complexity of the
  algorithm.

To determine the radius, we must first compute the \textit{characteristic sequences} $LR(A) = \{b_i\}$ and $RL(A) = \{c_i\}$:
$$b_i = \max\{a_0, a_1, \cdots, a_i\}\,, \quad c_i = \min\{a_i, a_{i+1}, \cdots, a_{n-1}\}\,, \quad 0 \leq i < n$$
Both sequences are easily computed in linear time and space using simple min/max prefix scans.
Note that $c_i \leq b_i \;\forall\; i$.
By performing a linear-time scan of these sequences and observing where they ``cross'' each other, we can compute
  the \textit{disorder measure} sequence $DM(A) = \{d_i\}$, where each $d_i$ represents how displaced $a_i$ is from its position
  in the sorted ordering of the elements of $A$:
$$d_i = \max\big\{\{i - j \mid c_i < b_j\} \cup \{0\} \big\}$$

The radius of $A$ is thus the maximum element from $DM(A)$. We employed a modification of the $DM$ algorithm from
  \supercite{altman89} where we save memory by simply keeping track of the maximum-encountered $d_i$ instead of storing the
  entire sequence; our sequential radius determination algorithm thus runs in linear time and requires $2n = \Theta(n)$ space.

The core of the sequential Roughsort implementation is the aforementioned radius halving algorithm from \supercite{altman89}.
This algorithm halves the radius $k$ of $A$ in place through the following three steps:
\begin{enumerate}\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item Partition each consecutive run of $k$ elements in $A$ about the mean of the run.
\item Starting at element $a_{\lfloor k/2 \rfloor}$ to stagger the partitions, repeat step 1 and go to 3.
\item Repeat step 1 and halt.
\end{enumerate}
A run of elements is ``partitioned'' by partially sorting it such that no element before the median of the run exceeds
  the median and no element thereafter is less than it.
We partition each run using the \texttt{nth_element} function from the  C\texttt{++} STL, which implements an optimized,
  linear-time selection algorithm\cite{clrs} such as
  Introselect\footnote{Introselect is based on Quickselect, which resembles a Quicksort
  that searches for the $n$th element of the sorted array by only partitioning one side of each pivot.}\cite{selectalg}.

Our sequential implementation thus sorts the array $A$ in place by determining its radius $k$ and halving the radius
  $\lg k$ times, in total requiring linear space and taking $\Theta(3(n/k \cdot k) \lg k) = \Theta(n \lg k)$ time.

The halving algorithm fails to halve the radius of $1$-sorted sequences, since it's impossible to stagger their radial runs;
  our implementation thus performs a single linear Bubblesort scan to quickly sort such sequences.

For an analysis of the sequential implementation's runtime performance, please see Section 9.

\section{Parallel Acceleration Using CUDA}
  The aforementioned algorithms assume a specific underlying memory in achieving the provided parallel implementation runtimes: the concurrent-read concurrent-write parallel random access model (CRCW PRAM).  Shiloach and Vishkin elaborate on this memory model, providing that it is a purely theoretical model which, inter alia, meets the following criteria: a shared, global memory of size m\textgreater p is available to p processors, and up to p positions in this memory may be simultaneously read or written in one step\cite{Shiloach1981}.  As with Roughsort, a number of algorithms take advantage of this model\textquoteright s assumptions to provide efficiency improvements over their serial counterparts\cite{Shiloach1982}\cite{raj1989}.

  Conspicuous, however, with these and other CRCW PRAM algorithms is a lack of presented implementations.  The model is merely that, and thus implementing such an algorithm requires a suitable proxy which shares the characteristics of CRCW PRAM.  In contemporary computing, the most notable such proxy is graphics processing unit (GPU) parallelization.

  GPUs invert a core principle of general purpose computing to yield a performance improvement on specific workloads.  Rather than having one or a handful of general purpose, powerful multiprocessors, GPUs opt for a different model: many less powerful, special purpose processors.  Contemporary examples include the Skylake Intel Xeon E3-1275 v5 CPU and the Nvidia GTX 1080 GPU.

  Nvidia--the manufacturer of the aforementioned graphics card--makes available an API suite and SDK which allows developers to readily access the GPU: CUDA.  This platform provides developers a simple interface with which they can offload processing of CUDA-augmented C/C++ software payloads to CUDA-supported Nvidia cards.

  The parallelism provided by Nvidia and CUDA in this scheme can be realized by developers who develop their payloads in specific ways.  Fundamentally, programs running on an Nvidia card via CUDA operate differently than their  CPU counterparts.  In general, a line of code executing on an Nvidia card is not processed in isolation.  Rather, the Nvidia CUDA scheme provides for single instruction, multiple thread (SIMT) execution, as opposed to the usual single instruction, single data scheme (SISD) provided by general purpose CPUs.  In contrast to a single instruction accessing or updating one memory location as the result of one instruction executing on a single core of a general purpose CPU, a single instruction executing on an Nvidia card is executing inside multiple threads on multiple respective cores simultaneously, each of which operates on a different location in memory.

  When a developer wants to take advantage of this parallelism, they structure their code such that the multiple results of the same instruction executing with values from different memory locations is a useful component of the overall program's payload.  Nvidia CUDA provides simple mechanisms by which to achieve this.

  The core component of an Nvidia payload is a thread; an instance of the series of instructions in the overall CUDA payload.  Multiple threads run in a group called a block.  Blocks contain a developer-specified number of threads each.  Between threads and blocks is the concept of a warp.  Warps are collections of 32 threads executing simultaneously on the same processor core on the Nvidia card.  Usually, all threads in a warp are executing the same instruction simultaneously. However, in situations where a conditional is evaluated by members of a warp, all threads for whom the conditional yields true will execute, and subsequently all threads with a false conditional result will execute, in a phenomenon called warp divergence.  This yields from the SIMT architecture, where all cores on which a warp executes must be processing the same instruction or temporarily switched off and not processing at all.  Blocks are divided up into multiple warps for scheduling and execution on an Nvidia GPU.  Collections of these blocks are referred to as a grid.  Finally, encompassing all of these components is the kernel.  Kernels are the series of instructions which threads, warps, blocks, and grids comprise an instance of and represent the sequence of code to actually be transferred to and executed on GPU.

  Nvidia provides a simple mechanism by which developers can access the parallelism provided by CUDA.  Each of the above divisions of the CUDA payload are provided with an identifier.  A tuple composed of the thread, block, and grid identifiers\footnote{Concerns with warps are abstracted away from the developer.  Consequently warp identifiers are not needed and are not available.} uniquely identifies that component of the workload.  Using this information, individual threads all executing the same instruction (such as a memory read from an array) can perform that instruction on distinct data (access different memory locations in the array) \cite{cuda}.


\section{Parallel Radius Determination}

  Using these techniques provided a platform to develop the GPU parallel versions of sequence radius finding and subsequent roughsorting.  In support of the above discussed parallel Roughsort implementation, CUDA implementations of the RL, LR, and DM sequences were developed.  This was done by assigning each thread a unique identifier and then, analogous to Altman and Chlebus\textquoteright work, having each thread manage the corresponding slot in the global unsorted array \cite{altman89}.  Each thread performed the parallel min- then max-prefix operations until the respective lists were efficiently determined ordered.  Sorting was determined by having each thread determine if its assigned slot in the LR or RL list was greater than the next element in the list, and having each thread globally assert any true (unsorted) result to global memory, available to all other threads.

  Once the RL and LR lists were completed, the DM list was computed by using Altman et. al.\textquoteright s serial radius techniques, while using the log(n) bounded techniques introduced by their RL and LR sequence determination algorithms.  However, a notable divergence from the serial version of the serial DM list algorithm was taken.  The inner loop of the serial DM list algorithm added a criteria which assured that B[i] (the ith element of the LR list) was always greater than or equal to C[i] (the ith element of the RL list).  As discussed, only the power of 2 greater than the actual value of k is actually needed for parallel Roughsort to function, so this criteria was removed in the interest of efficiency and simplicity\cite{altman89}.

  Finally, to determine the radius from the DM list, the maximum element needed to be found.  For this purpose, the Thrust CUDA library was used, which provides CUDA-optimized implementations of a number of C++ Standard Template Library (STL) functions\cite{thrust}.  One such STL function is the std::max\textunderscore element function.  Using the Thrust implementation of this function, maximum elements of large arrays can be found expediently on Nvidia hardware using CUDA\cite{extrema}.  Using this function yielded the maximum element of the DM list, which is k.  Further, to ensure all values quiesced during LR, RL, and DM list computations, a separate kernel was launched to perform the final DM computations.

\section{Radius Determination Runtime Analysis}

  Running these kernels yielded disappointing results, however.  For random lists composed together in such a manner as to yield a specific k value for the list (as discussed later in the paper)\footnote{Lists which had a specific k value were generated ahead of time and then evaluated using the radius finders to prove both correctness and provide for validity in this experimentation}, sequential CPU computations exceeded performance on GPU for any tested n when values of k exceeded 6, as see in Figure 2.\footnote{These results reversed when performed in a Windows environment, compiled with Visual Studio 2015 Community and run on commodity hardware.  The authors suspect this to be a false result and posit that optimization differences on the C++ STL nth\textunderscore element function are the cause.  This invites further investigation, however}.  Figure 1 reports more positive results for k equal to 2, showing that parallel run times were equalled or exceeded by sequential runtimes for tested values n equal to or in excess of 150,000, though this result was at best a roughly linear improvement over the sequential implementation and does not demonstrate the expected O(log(n)) runtime.

\begin{sidewaysfigure}
\input{plots/seqpar2}
\vspace{-4ex}
\caption{\label{fig:seqpar2}{\em
  Radius Determination Runtimes over Arrays of Length $n\cdot 10^6$, $k = 2$
}}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\input{plots/seqpar100}
\vspace{-4ex}
\caption{\label{fig:seqpar100}{\em
  Radius Determination Runtimes over Arrays of Length $n\cdot 10^6$, $k = 100$
}}
\end{sidewaysfigure}

\clearpage
\section{Parallel Roughsort Implementation and Results}

We first attempted to implement the parallel Roughsort described in \supercite{altman89}.
This approach follows the sequential version from Section 3, except in that the segment partitions are all launched in parallel
  at each of the 3 steps, synchronizing between steps.
We employed Nvidia's \textit{Thrust} library, which provides a parallel CUDA implementation of the C\texttt{++} STL.

Regrettably, Thrust has yet to provide an \texttt{nth_element} selection algorithm implementation, so we replaced it with calls
  to Thrust's \texttt{sort} algorithm, which performs a ``state-of-the-art'', highly-optimized radix sort
  implementation when invoked over arrays of primitive integer elements\cite{thrustradix, leischner09}.
Radix sort is a generalized counting sort that runs in linear time when operating over fixed-size array elements.
While radix sort requires additional storage (unlike Introselect, which was used in the sequential implementation), this
  additional storage ought to be acceptable as the segments we wish to partition ought to be relatively small.

Each Thrust \texttt{sort} call issues at least one kernel launch; we attempted to run radix sorts over the entire array in
  parallel by establishing a fixed number of CUDA \textit{streams}.
Whereas kernels launched through the same stream are executed in order, up to 16 kernels launched through distinct streams can be
  run concurrently on the GPU, available resources permitting\cite{cuda}.
We established a fixed number of CUDA streams and launched each \texttt{sort} call through them in a circular, round-robin
  fashion.

Unfortunately, while this implementation was very simple to code and correctly sorted its input, it was thousands of times
  slower than the sequential implementation.
Running the implementation through CUDA's \texttt{nvvp} profiler showed that the \texttt{sort} calls weren't being launched in
  parallel as expected, possibly due to some non-obvious synchronization mechanism that made concurrent launches of this
  function impossible.

We then implemented the parallel Roughsort approach described in \supercite{altman90}, which prescribes the parallel Mergesort
  algorithm by Cole in lieu of weak partitioning and is implemented as follows for sorting the array $A$ with radius $k$ in
  place:
\begin{enumerate}\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item In parallel, sort all consecutive runs of $2k$ elements in $A$.
\item Repeat step 1, but begin the first run at $a_k$; halt.
\end{enumerate}

Unlike the sequential algorithm, this approach fully sorts each array segment and thus needs only two batches of
  sorts without intra-radial staggering.
Furthermore, just a single iteration of this process suffices to fully sort the $k$-sorted input.

We implemented this approach using a CUDA kernel where each thread in the launch is responsible for sorting exactly one segment
  from step 1 or 2; the steps are divided into separate kernel launches, each given an offset from which begin segmenting the
  array.
While the threads in each launch run concurrently\footnote{For large arrays, the GPU can't host every thread simultaneously and
  must sequentially execute the thread blocks in batches.}, each thread sorts its designated segment sequentially.
In contrast to \supercite{altman90}, we used Thrust's radix sort instead of its Mergesort implementation in order to leverage
  the linear runtime of the former.
While this approach to the parallel Roughsort was far more successful than the previous, it still struggled against
  its competitors (see Section 9).

\section{Random $k$-Sorted Sequence Generation}
We developed a $k$-sorted random sequence generator to test our implementations.
The generator takes $k$ and the length of the desired array $n$ as arguments (specified by the user on the command line) and
  allocates a contiguous array of $n$ 32-bit random integers.
In order to avoid pseudorandom statistical weaknesses, we sourced these random integers from the high-end,
   cryptographically-secure hardware random number generator included in recent Intel processors\cite{intel12}.

We $k$-sort the random array $A$ by fully sorting it using the C\texttt{++} STL before perturbing the sorted elements.
This perturbance is achieved by randomly choosing sorted elements $a_p, a_q$ such that $q - p = k$ in order to guarantee the
  $k$-sortedness of the sequence.
To increase the rigor of the randomized $k$-sorting, each consecutive, disjoint run of at most $k + 1$ array elements
  (excluding $a_p$ and $a_q$) is then randomly shuffled to produce a random subarray that is itself at most $k$-sorted.

\section{Implementation Performance Analysis}
We tested our sequential and parallel Roughsort implementations against sequential and parallel Mergesort (as implemented by
  the C\texttt{++} STL and by CUDA Thrust, respectively) as well as against our simple, sequential Bubblesort implementation.
We chose Mergesort since it's a classic sort algorithm that also scales very well with parallel execution, capable
  of achieving a speedup of at least $\Theta(n / \lg^2 n)$ when given unlimited processors for a runtime of
  $\Theta(\lg^3 n)$\cite{clrs}.
While Bubblesort has a quadratic runtime upper bound, its lightweight, linear lower bound emerges when given nearly-sorted
  sequences and provides a means of comparing our implementation's performance for small values of $k$.

All tests were run on a workstation sporting a high-end, quad-core Intel CPU as well as an Nvidia GeForce GTX 1080---a
  recent and high-end\footnote{At the time of this writing.}, consumer-grade GPU suitable for heavy computational workloads
  over 32-bit values.

The time needed to transfer data to the GPU prior to running the parallel sorts isn't counted in the runtime results, since we'd
  expect such sorting operations to run as part of larger GPU workloads and not in isolation.
Since our parallel radius determiner wasn't complete until late in development and ran slower than the sequential
  implementation over arrays with larger radii, our parallel Roughsort implementation used the sequential implementation to
  compute $k$, the runtime of which wasn't figured into the results.

Figures 3 and 4 compare all five sorting algorithms, varying $n$ while fixing $k$ to 2 and 15, respectively. Roughsort ought
  to have a runtime advantage with the smaller radius that dissipates as we increase $k$.
Figures 5 and 6 invert the axes and vary $k$ while fixing $n$ at $750 \cdot 10^3$ and $1.25 \cdot 10^6$, respectively. 
For each tested $(n, k)$, all five sorters were run over the same, $k$-sorted random array of length $n$; their runtimes were
  averaged over eight such runs.
The runtime axis is logarithmic in every graph.

Figure 3 demonstrates the utility of the sequential Roughsort algorithm, which beats even the highly-optimized Mergesort from
  the C\texttt{++} STL at handling the nearly-sorted input arrays.
This victory is greatly overshadowed by the sequential Bubblesort, which is operating close to its optimal domain.

The next battery of tests in Figure 4 raises $k$ to 15, causing the sequential Roughsort's advantage over Mergesort to evaporate.
The parallel Roughsort generally outperforms Bubblesort here (which fairs better than expected against the sequential Mergesort
  in a trend that can't last for much longer to the right of the graph) and defeats the sequential Roughsort.
Unexpectedly, the parallel implementation exhibits about a factor-of-ten speedup over the $k = 2$ graph; this shows that
  the overhead of launching many CUDA threads over small array subsequences substantially counterbalances the greater runtime
  of sorting larger subsequences.

Figures 5 and 6 fix $n$ and vary $k$, yielding similar results.
For arrays of these lengths, the parallel Roughsort's crossover point lies somewhere around the interval $20 \leq k \leq 30$,
  where it manages to best the three sequential algorithms.
An invariant among all four graphs is the utter dominance of the parallel Mergesort implementation provided by Thrust,
  whose runtimes never depart from the $x$-axis.

These results suggest that our parallel Roughsort's constant runtime factor and crossover point are relatively high.
We were unable to observe a benefit from its superior asymptotic bound over nearly-sorted arrays that could fit within the
GPU's memory.

\begin{sidewaysfigure}
\input{plots/k2}
\vspace{-4ex}
\caption{\label{fig:k2}{\em
  Sort Runtimes over Arrays of Length $n\cdot 10^6$, $k = 2$
}}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\input{plots/k15}
\vspace{-4ex}
\caption{\label{fig:k15}{\em
  Sort Runtimes over Arrays of Length $n\cdot 10^6$, $k = 15$
}}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\input{plots/n750k}
\vspace{-4ex}
\caption{\label{fig:n750k}{\em
  Sort Runtimes over Arrays of Radius $k$, $n = 0.75\cdot 10^6$
}}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\input{plots/n1250k}
\vspace{-4ex}
\caption{\label{fig:n1250k}{\em
  Sort Runtimes over Arrays of Radius $k$, $n = 1.25\cdot 10^6$
}}
\end{sidewaysfigure}

\clearpage
\section{Explanation of Results}
  The source papers theorized that parallel algorithms would run at a O(log(n)) bound, so these contrary result must then be explained.  There are several possible reasons for this which should be considered, as there are similarities but also differences between GPU and the assumed CRCW PRAM.  Given many GPU processor threads may update multiple memory locations simultaneously, and likewise many memory location may be read in parallel on GPU, the underlying NVIDIA hardware that CUDA provides access to is reminiscent of CRCW PRAM.  However, GPUs differ from true CRCW PRAM in fundamental ways.  First, GPU memory is finite, while PRAM is of arbitrary size.  Furthermore, for GPU memory access times to provide the concurrent read and write characteristics reminiscent of PRAM, these memory operations must be performed on contiguous portions of GPU memory, whereas CRCW PRAM has no such ordering limitation.  Foremost in the differences between CRCW PRAM and GPU architectures however is the warp divergence concept addressed above.  What this implies for memory operations is that workloads exist which may prevent all but one thread in a warp from performing a memory operation at a particular time.  Consequently, there are practical limitations to what a GPU may achieve, and these limitations restrict the workloads which are practical to run on a GPU.  Not only must a GPU workload constrain its memory usage within practical limitations, the accesses to that memory must be orderly, and the underlying algorithm performing those accesses must be amenable to parallelization on GPU if the advantages of GPU processing over CPU processing are to be realized\cite{dehne2010exploring}. 

\section{Conclusion and Further Research}
An implementation of Cole's parallel median algorithm might enable a more efficient parallel Roughsort\cite{cole85}.

\clearpage
\bibliographystyle{plain}
\nocite{*}
\bibliography{refs} % bib database

\end{document}
