\documentclass[letterpaper, 12pt]{article}
\usepackage[letterpaper]{geometry}
\usepackage{amsmath, amssymb, url, rotating, sectsty, indentfirst}
\usepackage[doublespacing]{setspace}
\usepackage[strings]{underscore}

\sectionfont{\fontsize{12}{15}\selectfont}

\let\supercite\cite
\renewcommand{\cite}[1]{\textnormal{~\supercite{#1}}}

\title{Parallel Sorting of Roughly-Sorted Sequences\\CSCI 5172 Fall '16 Project}
\author{Anthony Pfaff, Jason Treadwell}

\begin{document}
\maketitle

Roughsort is a sorting algorithm that exploits the \textit{radius} of a sequence to beat the linearithmic
  lower runtime bound of the comparison sort family of algorithms.
Its structure invites implementation using parallel execution.
Here we present a parallel Roughsort implementation using Nvidia's CUDA platform for heterogeneous GPGPU programming and
  interpret our results.

\section{The Array Sorting Problem}
Sorting the elements of an array is among the most classic problems of computer science, often
  considered to be the most fundamental algorithmic problem\cite{clrs}.
Not merely just for arranging data to make it easier to read, sorting algorithms are frequently employed to render graphics
  and to improve the performance of other algorithms (e.g., set intersection or finding the unique elements of a list).

The \textit{comparison} sorts are an indispensable family of sorting algorithms containing such classics as Heapsort,
  Mergesort and Quicksort.
True to its name, algorithms in this family determine how to reorder a sequence by comparing its elements against each other
  according to some natural or explicitly-provided ordering;
  for the sake of simplicity, we shall only consider the sorting of integral elements in nondecreasing order, backed by
  random-access array storage\footnote{Sequential-access storage demands distinct and divergent design considerations.}.
A well-established result is the ``linearithmic'' $\Omega(n \lg n)$ runtime lower bound for the general problem of sorting
  an array of $n$ elements by comparison; algorithms such as Mergesort achieve $\Theta(n \lg n)$ runtime (and are thus
  asymptotically optimal) whereas Quicksort, while usually outperforming Mergesort over arrays that fit in memory,
  has quadratic-runtime pathological cases.

Comparison-based sorting algorithms, such as Mergesort, often have divide-and-conquer structures that can easily and
  substantially be sped up by using \textit{nested parallelism} and other means of parallel execution\cite{clrs}.

\section{Sorting Roughly-Sorted Sequences}
The optimal $\Omega(n \lg n)$ runtime bound of comparison sorting can be beaten when additional analysis is performed on the
  input sequence\cite{clrs}.
An algorithm by Altman and Igarashi, which we call \textit{Roughsort}, improves upon this lower bound by exploiting the degree
  to which the unprocessed input array is partially sorted\cite{altman89}.
Specifically, Roughsort sorts an array in $\Theta(n \lg k)$ time, where $k$ is the \textit{radius} of the input sequence.

A sequence $A = \{a_0, a_1, \cdots, a_{n-1}\}$ is $k$-sorted if it satisfies
$$a_i \leq a_j \,,\quad i < j - k \quad\forall\; 0 \leq i \leq j \leq n-1\,,$$
where a 0-sorted sequence is fully-sorted.
The radius of $A$ is the smallest such $k$ for which $A$ is $k$-sorted.
Alternatively, and perhaps more usefully, the radius of $A$ measures how far away its most out-of-order element is from its
  position in the fully-sorted array:
$$\text{radius}(A) = \max\{j - i \mid j > i,\, a_i > a_j \} \,.$$

Roughsort exploits the radius of $A$ by using a \textit{halving} algorithm that takes a $2l$-sorted sequence and partially
  sorts it into an $\lfloor l - 1 \rfloor$-sorted sequence.
By repeatedly halving the sortedness of $A$, we fully sort it in $\lg k$ runs, whence the runtime.
The radius of $A$ must therefore be given \textit{a priori} or determined from the input
  in order to effectively perform the algorithm.

Some applications involve sorted arrays that are updated or extended in such a way as to only slightly perturb the ordering
  (i.e., the radius $k$ of an out-of-order sequence is small)\cite{altman89}.
In such cases, Roughsort could be employed to quickly resort the sequences faster than by using a full $\Theta(n \lg n)$
  comparison sort.
Since an unsorted sequence of length $n$ can be at most $(n - 1)$-sorted, the runtime of Roughsort is
  $\Theta(n \lg k) = O(n \lg n)$ and is thus asymptotically-optimal among comparison-based sorting algorithms.

Similar to other comparison sorts, Roughsort has divide-and-conquer characteristics that support parallel execution on various
  multiprocessor models\cite{altman89, altman90}.

\section{Sequential Implementation}
As noted, Roughsort must know the radius $k$ of the input array $A = \{a_0, a_1, \cdots, a_{n-1}\}$
  in order to completely sort it.
If not known, the radius can be determined in linear time\cite{altman89}, preserving the $\Theta(n \lg k)$ complexity of the
  algorithm.

To determine the radius, we must first compute the $LR(A) = \{b_i\}$ and $RL(A) = \{c_i\}$ \textit{characteristic sequences}:
$$b_i = \max\{a_0, a_1, \cdots, a_i\}\,, \quad c_i = \min\{a_i, a_{i+1}, \cdots, a_{n-1}\}\,, \quad 0 \leq i < n$$
Both sequences are easily computed in linear time and space using simple min/max prefix scans.
Note that $c_i \leq b_i \;\forall\; i$.
By performing a linear-time scan of these sequences and observing where they ``cross'' each other, we can compute
  the \textit{disorder measure} sequence $DM(A) = \{d_i\}$, where each $d_i$ represents how displaced $a_i$ is from its position
  in the sorted ordering of the elements of $A$:
$$d_i = \max\big\{\{i - j \mid c_i < b_j\} \cup \{0\} \big\}$$

The radius of $A$ is thus the maximum element from $DM(A)$. We employ a modification of the $DM$ algorithm from
  \supercite{altman89} where we save memory by simply keeping track of the maximum-encountered $d_i$ instead of storing the
  entire sequence; our sequential radius determination algorithm thus runs in linear time and requires $2n = \Theta(n)$ space.

The core of the sequential Roughsort implementation is the aforementioned radius halving algorithm from \supercite{altman89}.
This algorithm halves the radius $k$ of $A$ in place through the following three steps:
\begin{enumerate}\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item Partition each consecutive run of $k$ elements in $A$ about the mean of the run.
\item Starting at element $a_{\lfloor k/2 \rfloor}$ to stagger the partitions, repeat step 1 and go to 3.
\item Repeat step 1 and halt.
\end{enumerate}
A run of elements is ``partitioned'' by partially sorting it such that no element before the median of the run exceeds
  the median and no element thereafter is less than it.
We partition each run using the \texttt{nth_element} function from the  C\texttt{++} STL, which performs an optimized,
  linear-time selection algorithm\cite{clrs} such as
  Introselect\footnote{Introselect is based on Quickselect, which resembles a Quicksort
  that searches for the $n$th element of the sorted array by only partitioning one side of each pivot.}\cite{selectalg}.

Our sequential implementation thus sorts the array $A$ in place by determining its radius $k$ and halving the radius
  $\lg k$ times, in total requiring linear space and taking $\Theta(3(n/k \cdot k) \lg k) = \Theta(n \lg k)$ time.

The halving algorithm fails to halve the radius of $1$-sorted sequences, since it's impossible to stagger their radial runs;
  our implementation thus performs a single linear Bubblesort scan to quickly sort such sequences.

For an analysis of the sequential implementation's runtime performance, please see Section 7.

\section{Parallel Acceleration Using CUDA}

\section{Parallel Radius Determination}

\begin{sidewaysfigure}
\input{plots/seqpar2}
\vspace{-4ex}
\caption{\label{fig:seqpar2}{\em
  Radius Determination Runtimes over Arrays of Length $n\cdot 10^6$, $k = 2$
}}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\input{plots/seqpar100}
\vspace{-4ex}
\caption{\label{fig:seqpar100}{\em
  Radius Determination Runtimes over Arrays of Length $n\cdot 10^6$, $k = 100$
}}
\end{sidewaysfigure}

\clearpage
\section{Parallel Roughsort Implementation and Results}

We first attempted to implement the parallel approach described in \supercite{altman89}.
This approach follows the sequential version from Section 3, except in that the segment partitions are all launched in parallel
  at each of the 3 steps, synchronizing between steps.
We employed Nvidia's \textit{Thrust} library, which provides parallel CUDA implementations of the C\texttt{++} STL.

Regrettably, Thrust has yet to provide an \texttt{nth_element} selection algorithm implementation, so we replaced it with calls
  to Thrust's \texttt{sort} algorithm, which performs a ``state-of-the-art'', highly-optimized radix sort
  implementation when invoked over arrays of primitive integer elements\cite{thrustradix, leischner09}.
Radix sort is a generalized counting sort that runs in linear time when operating over fixed-size array elements.
While radix sort requires additional storage (unlike Introselect, which was used in the sequential implementation), this
  additional storage ought to be acceptable as the segments we wish to partition ought to be relatively small.

Each Thrust \texttt{sort} call issues at least one kernel launch; we attempted to run radix sorts over the entire array in
  parallel by establishing a fixed number of CUDA \textit{streams}.
Whereas kernels launched through the same stream are executed in order, up to 16 kernels launched through distinct streams can be
  run concurrently on the GPU, available resources permitting\cite{cuda}.
We established a fixed number of CUDA streams and launched each \texttt{sort} call through them in a circular, round-robin
  fashion.

Unfortunately, while this implementation was very simple to code and correctly sorted its input, it was thousands of times
  slower than the sequential implementation.
Running the implementation through CUDA's \texttt{nvvp} provider showed that the \texttt{sort} calls weren't being launched in
  parallel as expected, possibly due to some non-obvious synchronization mechanism that made concurrent launches of this
  function impossible.

We then implemented the parallel Roughsort approach described in \supercite{altman90}, which prescribes the parallel Mergesort
  algorithm by Cole in lieu of weak partitions and is implemented as follows for sorting the array $A$ with radius $k$ in place:
\begin{enumerate}\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item In parallel, sort all consecutive runs of $2k$ elements in $A$.
\item Repeat step 1, but begin the first run at $a_k$; halt.
\end{enumerate}

Unlike the sequential algorithm, this approach employs full sorting of the array segments and thus needs only two batches of
  sorts without intra-radial staggering.
Furthermore, just a single iteration of this process suffices to fully sort the $k$-sorted input.

We implemented this approach using a CUDA kernel where each thread in the launch is responsible for sorting exactly one segment
  from step 1 or 2; the steps are divided into separate kernel launches, each given an offset from which begin segmenting the
  array.
While the threads in each launch run concurrently\footnote{For large arrays, the GPU can't host every thread simultaneously and
  must sequentially execute the thread blocks in batches.}, each thread sorts its designated segment sequentially.
In contrast to \supercite{altman90}, we used Thrust's radix sort instead of its Mergesort implementation in order to leverage
  the linear runtime of the former.
While this approach to the parallel Roughsort was far more successful than the previous, it still competed more poorly than
  its competitors (see Section 7).

\section{Random $k$-Sorted Sequence Generation}
We developed a $k$-sorted random sequence generator to test our implementations.
The generator takes $k$ and the length of the desired array $n$ as arguments (specified by the user on the command line) and
  allocates a contiguous array of $n$ 32-bit random integers.
In order to avoid pseudorandom statistical weaknesses, we sourced these random integers from the high-end,
   cryptographically-secure hardware random number generator included in recent Intel processors\cite{intel12}.

We $k$-sort the random array $A$ by fully sorting it using the C\texttt{++} STL before perturbing the sorted elements.
This perturbance is achieved by randomly choosing sorted elements $a_p, a_q$ such that $q - p = k$ in order to guarantee the
  $k$-sortedness of the sequence.
To increase the rigor of the randomized $k$-sorting, each consecutive, disjoint run of at most $k + 1$ array elements
  (excluding $a_p$ and $a_q$) is then randomly shuffled to produce a random subarray that is itself at most $k$-sorted.

\section{Implementation Performance Analysis}
mergesort speedup
upload times
ignore rad gen

\begin{sidewaysfigure}
\input{plots/k2}
\vspace{-4ex}
\caption{\label{fig:k2}{\em
  Sort Runtimes over Arrays of Length $n\cdot 10^6$, $k = 2$
}}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\input{plots/k15}
\vspace{-4ex}
\caption{\label{fig:k15}{\em
  Sort Runtimes over Arrays of Length $n\cdot 10^6$, $k = 15$
}}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\input{plots/n750k}
\vspace{-4ex}
\caption{\label{fig:n750k}{\em
  Sort Runtimes over Arrays of Radius $k$, $n = 0.75\cdot 10^6$
}}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\input{plots/n1250k}
\vspace{-4ex}
\caption{\label{fig:n1250k}{\em
  Sort Runtimes over Arrays of Radius $k$, $n = 1.25\cdot 10^6$
}}
\end{sidewaysfigure}

\clearpage
\section{Explanation of Results}

\section{Conclusion and Further Research}

\clearpage
\bibliographystyle{plain}
\nocite{*}
\bibliography{refs} % bib database

\end{document}
