\documentclass[10pt, xcolor={dvipsnames}, aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[super, square]{natbib}
\usepackage{mathtools, relsize, graphicx, amssymb, amsthm}
\usepackage{listings}
\usepackage{color}
\usepackage[strings]{underscore}

\lstset{
    frame=single,
    basicstyle=\footnotesize,
    keywordstyle=\color{purple},
    numbers=left,
    numbersep=5pt,
    showstringspaces=false, 
    stringstyle=\color{blue},
    tabsize=4,
    language=C++
}

\mode<presentation>{\usetheme{Berlin} \usecolortheme{beetle}}

\title{Parallel Sorting of Roughly-Sorted Sequences}
\author{Anthony Pfaff, Jason Treadwell}
\institute{CSCI 5172 $|$ CU Denver $|$ Fall '16}
\date{12.10.2016}

\begin{document}
\setbeamertemplate{navigation symbols}{} %remove navigation symbols
\bibliographystyle{plain}
\nocite{*}
% gets rid of the references button
\renewcommand{\bibsection}{\subsubsection*{\bibname}}
\graphicspath{{./}}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\transfade
\frametitle{Introduction}
Sorting a sequence according to some ordering is a foundational problem of computer science. \newline

Sorting has crucial and non-obvious applications. There exist a great many such algorithms. \newline

The optimal runtime of sorting a sequence of length $n$ by \textit{comparison} is $\Theta(n \lg n)$.
\end{frame}

\begin{frame}
\frametitle{Sort Algorithms}
The comparison sorts include Mergesort, Heapsort, Quicksort, etc.\ and have diverse strengths. \newline

The former two are asymptotically optimal; Quicksort frequently outruns them, but is $O(n^2)$. \newline

We can beat the linearithmic runtime bounds through additional analysis of the input array.
\end{frame}

\begin{frame}
\frametitle{Roughly-Sorted Sequences}
The sequence $A = \{a_0, a_1, \cdots, a_{n-1}\}$ is $k$-sorted if it satisfies
$$a_i \leq a_j \quad\forall\; i < j - k\,,\quad 0 \leq i \leq j < n \,.$$ \newline

The radius of $A$ is the smallest $k$ such that $A$ is $k$-sorted:
$$\text{radius}(A) = \max\{j - i \mid j > i,\, a_i > a_j \} \,.$$
\end{frame}

\begin{frame}
\frametitle{Conventions}
We'll only consider sequences of 32-bit integers backed by \textit{random-access arrays}. \newline

The array $A = \{a_0, a_1, \cdots, a_{n-1}\}$ is \textit{sorted} if its elements are all ordered in nondecreasing order. \newline

When we say $A$ is $k$-sorted, we imply that $k$ is minimal (i.e., the radius of $A$ is $k$).
\end{frame}

\begin{frame}
\frametitle{Roughly-Sorted Sequences (cont.)}
Some applications involve sorted arrays that become slightly perturbed. \newline

We might be able to exploit their partial sortedness to beat the $\Omega(n \lg n)$ lower runtime bound. \newline

Comparison-based sorts are prone to divide-and-conquer and rife with opportunities for parallel speedup.
\end{frame}

\begin{frame}
\frametitle{Roughsort}
Roughsort exploits the presortedness of the array $A$ by applying a radius \textit{halving} algorithm. \newline

The algorithm sorts $A$ by halving its radius $\lg k$ times in runtime $\Theta(n \lg k) = O(n \lg n)$. \newline

Roughsort does invite parallel speedup, yet must determine $k$ to effectively sort $A$.
\end{frame}

\begin{frame}
\frametitle{Determining the Radius}
Using min/max prefix scans, we compute the \textit{characteristic sequences} of $A$:
$$LR(A) = \{b_i\}\,,\quad RL(A) = \{c_i\}\,,\quad 0 \leq i < n\,,$$
$$b_i = \max\{a_0, a_1, \cdots, a_i\}\,,\quad c_i = \min\{a_i, a_{i+1}, \cdots, a_{n-1}\}\,.$$

The radius $k$ of $A$ is the maximum element from the \textit{disorder measure} of $A$:
$$DM(A) = \{d_i\}\,,\quad d_i = \max\big\{\{i - j \mid c_i < b_j\} \cup \{0\} \big\}\,,\quad k = \max DM(A) \,.$$

Finding $k$ thus takes linear time and space, preserving the $\Theta(n \lg k)$ complexity of Roughsort.
\end{frame}

\begin{frame}
\frametitle{Halving the Radius}
\begin{enumerate}
\item Partition each consecutive run of $k$ elements in $A$ about the mean of the run.
\item Starting at element $a_{\lfloor k/2 \rfloor}$ to stagger the partitions, repeat step 1 and go to 3.
\item Repeat step 1 and halt.
\end{enumerate}

Each partition is performed using STL's \texttt{nth_element()} and takes linear time. \newline

Our sequential implementation thus sorts $A$ in place by halving its radius $\lg k$ times, taking linear space and
  $\Theta(3(n/k \cdot k) \lg k) = \Theta(n \lg k)$ time.
\end{frame}

% BEGIN CUDA INTRO

\begin{frame}
\frametitle{Parallelization}
Roughsort may be parallelized, but assumes underlying CRCW PRAM to achieve this.
\newline\newline
CRCW PRAM:
\begin{enumerate}
\item Model for RAM which essentially says multiple slots in memory may be updated or concurrently
\item This update is done by a fixed number of processors
\item Several other restrictions to the model
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{CUDA: Our Answer for CRCW PRAM}
However, CRCW PRAM is just a useful abstraction.  The real world is just that: real
\newline\newline
Must find a proxy for CRCW PRAM to develop an implementation.
\end{frame}

\begin{frame}
\frametitle{CUDA: Our Answer for CRCW PRAM (2)}
Enter CUDA.  Similar characteristics:
\begin{enumerate}
\item Multiple slots in global memory may be updated simultaneously
\item Multiple slots may also be read simultaneously
\item Thousands more processor cores than standard CPUs, providing a mechanism to update all this parallel memory
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{CUDA: Background}
Nvidia provides APIs and an SDK to allow developers the use of GPU functions.
\newline\newline
Model:
\begin{enumerate}
	\item CPU-homed (host) program is specifically programmed to talk to GPU
	\item Host program is compiled ahead of time with CUDA hooks and calls embedded in its C++ source
	\item Payload data is explicitly shipped off to the GPU
	\item Host ships the compiled GPU code (kernel) to the GPU and launches it asynchronously
	\item GPU executes payload, places data in defined locations, host program picks up result data
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{CUDA: Background(2)}
Organization
\begin{enumerate}
	\item Core component and lowest-level work unit of CUDA execution context is a thread
	\item Up to thousands of threads may be organized in a group called a block
	\item Multiple blocks make up a grid
	\item Threads per block and other parameters are specified by launching program
	\item CUDA cheaply switches threads in and out of processing on cores
\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{CUDA: Background(3)}
	Warps
	\begin{enumerate}
		\item Groups of 32 threads which execute together are called a warp - SIMT
		\item This is a concept that is abstracted away from programmer during development/kernel launch (32 is fixed)
		\item Threads in a warp executing on a group of 32 cores are all executing the same instruction at the same time, optimally
		\item In cases of a conditional, some cores may pause to allow threads with a true value to process, then false values process - Warp divergence
	\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{CUDA: Pattern}

Threads, blocks, and grid all have identifiers
\newline\newline
Developer references these identifiers in code
\newline\newline
Using an understanding of how work is to be divided up among threads, these identifiers can inform a thread what work it needs to do

\end{frame}

\begin{frame}
	\frametitle{CUDA: Visual}
	\begin{figure}
	\centering
	\includegraphics[height=.7\textheight]{./cudaorg.png}
	\newline
	\textbf{Credit: CUDA C Programming Guide}
	\end{figure}
\end{frame}
% END CUDA INTRO

% BEGIN PAR RADIUS
\begin{frame}
\frametitle{Parallel Radius Finding: Translating}
Back to parallel Roughsort: we know we can find the radius in parallel.  Merely need to convert the pseudocode to CUDA code.
\newline\newline
Straightforward for $LR$ list, slightly less so for $RL$, least straightforward for $DM$.
\newline\newline
Eventually settled on a $DM$ list determination based on the sequential, with array indices fixed to thread ID.
\newline\newline
Added an efficient method to check for full array sortedness as part of the $RL$ and $LR$ list components.
\end{frame}


\begin{frame}
	\frametitle{Parallel Radius Finding: Scaling Up}
Testing:
\begin{enumerate}
	\item Worked fine for a low values
	\item Did not function at all for large values of $n$ (widely inaccurate $k$ values)
\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Parallel Radius Finding: Re-Visiting}
	Cause/Fix:
	\begin{enumerate}
		\item Asked individual threads to allocate their slot in the global memory $RL$ and $LR$ lists upon startup
		\item Not all threads are immediately scheduled for large thread counts--bug
		\item Subsequently started copying all values before kernel launch
	\end{enumerate}
	
\end{frame}

\begin{frame}
	\frametitle{Parallel Radius Finding: Scaling Up (2)}
	After fixing, $DM$ list was correct to an extent, but it was slow.
	\newline\newline
	Resolved this by realizing only need our "radius" to approximate the real $k$ value, in that we just need the next power of two greater
	\newline\newline
	Added \texttt{Thrust max_element} code to find the greatest value in the $DM$ list, thus yielding our radius
\end{frame}
% END PAR RADIUS

% BEGIN PAR RADIUS RESULTS
\begin{frame}
\frametitle{Parallel Radius Finding: Results ($k = 2$)}
\begin{figure}
\resizebox{!}{.8\textheight}{\input{./plots/seqpar2.tex}}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Parallel Radius Finding: Results($k = 100$)}
\begin{figure}
	\resizebox{!}{.8\textheight}{\input{./plots/seqpar100.tex}}
\end{figure}
\end{frame}
% END PAR RADIUS RESULTS

\begin{frame}
\frametitle{Parallel Roughsort: Failed Approach}
Our first implementation simply parallelized the halving algorithm of our sequential Roughsort. \newline

We replaced \texttt{nth_element()} with Thrust's \texttt{sort()}, an optimized radix sort. \newline

We tried to launch every partitioning in parallel using \textit{streams} and got abysmal results.
\end{frame}

\begin{frame}
\frametitle{Parallel Roughsort: Better Approach}
\begin{enumerate}\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item In parallel, \textbf{sort} all consecutive runs of $2k$ elements in $A$.
\item Repeat step 1, but begin the first run at $a_k$; halt.
\end{enumerate}
This process sorts each segment and needs only a single iteration to fully sort $A$. \newline

We implemented it as a CUDA kernel where the array segments were divided up among many parallel threads,
  but each thread sequentially sorted its designated array segment.
\end{frame}

\begin{frame}
\frametitle{Testing the Implementation}
We tested our Roughsort implementations against sequential and parallel Mergesort as well as against Bubblesort. \newline

Each test generated a random $k$-sorted array for some given $k$ and length $n$, then fed it to all five algorithms, averaging
  their results over eight runs. Each array was sourced by a hardware RNG and sorted before being
  $k$-perturbed and shuffled. \newline

The tests were run on a workstation with a high-end, quad-core Xeon CPU as well as a high-end, consumer-grade
  Nvidia GeForce 1080 GTX GPU suitable for workloads over 32-bit array elements.
\end{frame}

\begin{frame}
\frametitle{Sort Runtimes $(k = 2)$}
\begin{figure}
\scalebox{0.4}{\input{plots/k2}}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Sort Runtimes $(k = 15)$}
\begin{figure}
\scalebox{0.4}{\input{plots/k15}}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Sort Runtimes $(n = 0.75 \cdot 10^6)$}
\begin{figure}
\scalebox{0.4}{\input{plots/n750k}}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Sort Runtimes $(n = 1.25 \cdot 10^6)$}
\begin{figure}
\scalebox{0.4}{\input{plots/n1250k}}
\end{figure}
\end{frame}

% BEGIN EXPLANATION OF BAD PAR. PERFORMANCE
\begin{frame}
\frametitle{Parallel Performance}
Below expectations:
\begin{enumerate}
	\item Parallel radius runs in linear time
	\item Parallel Roughsort by Mergesort consistently
\end{enumerate}
Why: CUDA is not CRCW PRAM
\begin{enumerate}
	\item Warp divergence
	\item Memory coalescence
\end{enumerate}
\end{frame}
% END EXPLANATION OF BAD PAR. PERFORMANCE

\begin{frame}
\frametitle{Conclusion and Further Research}
Demonstrated a good method for evaluating sorting algorithms, by being able to generate $k$-sorted sequences
\newline\newline
Developed a first implementation of Roughsort\textemdash to be continued by others
\newline\newline
Bottlenecks in implementation should be researched so they may be resolved
\end{frame}
\end{document}
